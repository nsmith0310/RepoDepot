{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b3e8f91-4b1a-49e7-9009-d311dab7d51a",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 42px;\"><b>Recurrent Complex-Weighted Autoencoder</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b71cb-d7ea-4872-a44b-09a774368f29",
   "metadata": {},
   "source": [
    "Based on the paper <a href=\"https://www.proceedings.com/079017-4468.html\">Recurrent Complex-Weighted Autoencoders for Unsupervised Object Discovery</a> featured in NeurIPS 2024<br>\n",
    "Annotated by N. Smith, J. Kim, and S. Boinpally\n",
    "\n",
    "The original paper was authored by:\n",
    "\n",
    "<b>Anand Gopalakrishnan:</b><br> \n",
    "The Swiss AI Lab, IDSIA, USI & SUPSI, Lugano, Switzerland\n",
    "\n",
    "<b>Aleksandar Stanic, Michael Curtis Mozer:</b><br>\n",
    "Google DeepMind\n",
    "\n",
    "<b>Jürgen Schmidhuber:</b><br>\n",
    "The Swiss AI Lab, IDSIA, USI & SUPSI, Lugano, Switzerland<br>\n",
    "AI Initiative, KAUST, Thuwal, Saudi Arabia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9bc4de7-6c12-4aac-8d3f-9eaf5a8b13dc",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<p style=\"font-size: 24px;\"><b>Background</b></p>\n",
    "The authors' recurrent complex-weighted autoencoder model SynCx was developed to tackle the binding problem using unsupervised learning. The binding problem is a domain of object-centric learning wherein the goal is to associate input values amongst themselves and can be thought of as being similar to clustering. The goal is to group input values which belong to the same object in the environment together in a representation. Consider the still-life photograph in <b>figure 1</b>.\n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://lh3.googleusercontent.com/d/1-LRqFIi2xmEfM3YNZsO6knjeNykmdr5z\" style=\"display:block; margin:auto\"/>\n",
    "<br>\n",
    "<b>Figure 1</b> Image source: <a href=\"https://weedit.photos/lighting-for-still-life-photography/\">https://weedit.photos/lighting-for-still-life-photography/</a><br><br></div></center>\n",
    "<br><br>\n",
    "\n",
    "A computer receiving the raw version of this image does not know the pixels from the yellow pitcher belong to one object, the pixels from the blue bowl to another object, and so on. The goal of object-centric learning is to both separate inputs (pixels in this case) from one another if they do not belong to the same object while also grouping inputs which belong to the same object together. <b>Figure 2</b> shows one possible mask an object-centric model could produce as output from the still-life in <b>figure 1</b>. Extraneous information like texture and shadowing have been stripped away with the output only indicating what object each pixel is associated with - table, pitcher, bowl, each fruit, and the background. This example would be a perfect or near-perfect output.\n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://lh3.googleusercontent.com/d/1vPdgDU5CNSh2d9yPwtzZnvHyqzp8Llcb\" style=\"display:block; margin:auto\"/>\n",
    "<br>\n",
    "<b>Figure 2</b> <br><br></div></center>\n",
    "<br><br>\n",
    "\n",
    "<b>Figure 3</b> shows a less-than-perfect output with some of the pixels having been misassigned to the wrong objects. Those familiar with semantic segmentation may recognize the output as resembling both the masks and output from semantic segmentation models. Though similar in appearance, the model proposed by the authors' is entirely unsupervised in its training process. \n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://lh3.googleusercontent.com/d/1mTykMWlnYwdtSb9f8ivOux-Fg2e4YVtc\" style=\"display:block; margin:auto\"/>\n",
    "<br>\n",
    "<b>Figure 3</b> <br><br></div></center>\n",
    "<br><br>\n",
    "\n",
    "The recurrent complex-weighted autoencoder model processes input and produces object bindings entirely using complex numbers. The primary innovation of the recurrent complex-weighted autoencoder is that all operations use complex numbers only and that it does not use gating mechanisms to implement binding. It is a synchrony-based model which differed from the predominant paradigm in object-centric learning, namely slot-based models. Slot-based models maintain latent activation vectors - slots - which encode the features of objects, one object per slot. Synchrony-based models like this one instead encode bindings based on the synchronization of features which are phases in this case. Prior work relied upon real-valued weights which meant it was not able to take full advantage of the representational power of complex numbers. Additionally, the gating mechanisms employed by prior work made assumptions which limited their generalizability. As the model works with complex numbers, what follows is a brief refresher or introduction to complex numbers:\n",
    "\n",
    "\n",
    "Imaginary unit $i = \\sqrt{-1}$<br><br>\n",
    "Complex number $z$:<br>\n",
    "Rectangular or Cartesian form: $x$ is the real component while $y$ is the imaginary component<br>\n",
    "$z = x + iy$<br>\n",
    "magnitude: $r = \\sqrt{x^2 + y^2}$<br>\n",
    "phase: $\\phi = \\tan^{-1}(\\frac{y}{x})$<br><br> \n",
    "Polar form: phase $\\phi$ is the angle between the real axis and the complex number vector<br>\n",
    "$z = r(cos(\\phi) + isin(\\phi))$<br>\n",
    "$z = re^{i\\phi}$ because $e^{i\\phi} = cos(\\phi) + isin(\\phi)$ using Euler's identity<br> \n",
    "$x = cos(\\phi)$<br>\n",
    "$y = sin(\\phi)$<br>\n",
    "\n",
    "\n",
    "\n",
    "The recurrent complex-weighted autoencoder encodes object bindings using the phase of complex numbers. Each pixel of the input images is assigned a phase and this phase tracks what object the pixel is bound to or associated with. Pixels belonging to the same object should have aligned phases. The model is recurrent because it is run on the same input some number of times, where the phase output from the prior iteration is used in the subsequent iteration. When input images are passed to the model, they are first converted to complex form using randomly initialized phases drawn from a Von Mises distribution with mean zero with concentration equal to one. The model is trained to reconstruct input images which are expressed as the magnitude of the output, though the phase output is the real objective. The model is trained using the average mean squared error over the magnitude output of all recurrent iterations or in other words, mean MSE. The phase output from the last iteration can then be used to recover object bindings. This is done by clustering the phases using K-Means clustering. Recovery of the bindings depends on having a sense of how many distinct objects are in the input, though the training process does not depend on this knowledge and is entirely unsupervised.<br><br>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb9e3d-09ba-43dc-9a5c-245b4d1bbf7d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<p style=\"font-size: 24px;\"><b>Model implementation</b></p><br>\n",
    "<p style=\"font-size: 18px;\"><b>Necessary imports and global context settings</b></p><br>\n",
    "The following code imports all libraries needed for the remainder of the implementation and sets the seeds used during random number generation<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a22d33-c53e-41ca-971d-d2c6d95de403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# Global setup\n",
    "# ============================================================\n",
    "\n",
    "#set device; in order to utilize GPU, please change \"cpu\" to \"cuda\"\n",
    "using_device = \"cpu\"\n",
    "device = torch.device(using_device)\n",
    "\n",
    "#set numpy random number generator seed for reproducability\n",
    "numpy_seed = 17\n",
    "np.random.seed(numpy_seed)\n",
    "\n",
    "#set pytorch random number generator seed for reproducability\n",
    "torch_seed = 1\n",
    "torch.manual_seed(torch_seed); None\n",
    "\n",
    "#Ensure deterministic CUDA behavior\n",
    "if using_device == \"cuda\":\n",
    "    torch.cuda.manual_seed(torch_seed); None\n",
    "    torch.cuda.manual_seed_all(torch_seed); None\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee939f-7ce0-4613-a2f8-288c6b4f9831",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<p style=\"font-size: 18px;\"><b>Helper functions</b></p><br>\n",
    "The following code defines various helper functions needed throughout the duration of the model implementation. The functions in their order of appearance in the code are:\n",
    "<ul>\n",
    "<li><b>to_cartesian():</b> This function converts polar output from the model to Cartesian output which is needed before clustering using K-Means. This function follows the methodology in the source paper and converts the magnitudes of all object pixels to unit magnitude and sets the magnitudes of all background pixels to zero.</li>\n",
    "<li><b>angle():</b> This function numerically stablizes the phase components of model output. </li>\n",
    "<li><b>cluster():</b> This function is used to cluster the output of the model. K-Means clustering is used to cluster prior to computing ARI score.</li>\n",
    "<li><b>ari_score():</b> This function computes the average ARI score by calling <b>ari()</b> on each model output.</li>    \n",
    "<li><b>convert_data():</b> This function converts raw images and their corresponding masks to tensors and normalizes the images.</li>\n",
    "<li><b>visualize():</b> This function is used to visualize the model's output.</li>\n",
    "<li><b>ari():</b> This function computes individual Adjusted Rand Indices (ARI) and is called by <b>ari_score()</b> to averages them.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ee5a1-4dca-4249-a373-c90ffddec98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Helper functions\n",
    "# These functions are used for:\n",
    "# - converting complex model outputs into cartesian vectors\n",
    "# - clustering complex model outputs and computing ARI\n",
    "# - visualization\n",
    "# - preparing data format\n",
    "# ============================================================\n",
    "\n",
    "#convert polar output to cartesian form\n",
    "#set all magnitudes to 1 for objects using mask; set all magnitudes\n",
    "#to 0 for the background using mask\n",
    "def to_cartesian(model_output,masks):\n",
    "    masks = torch.squeeze(masks,1)\n",
    "    magnitude = model_output.abs()\n",
    "    phase = model_output.angle()\n",
    "\n",
    "    # Stack magnitude=1 and phase channels\n",
    "    magnitude_and_phase = torch.cat((torch.ones_like(phase)[:, None], phase[:, None]), axis=1)\n",
    "\n",
    "     # Zero out background by using mask\n",
    "    magnitude = 1. - (masks==0.).type(torch.float)[:, None]\n",
    "    magnitude_and_phase = magnitude_and_phase * magnitude[:, None]\n",
    "\n",
    "    # Convert polar → cartesian\n",
    "    output = torch.zeros(magnitude_and_phase.shape)\n",
    "    r = magnitude_and_phase[:, 0]\n",
    "    phi = magnitude_and_phase[:, 1:]\n",
    "    sin = 1\n",
    "    for i in range(magnitude_and_phase.shape[1] - 1):\n",
    "        output[:, i] = r * torch.cos(phi[:, i]) * sin\n",
    "        sin = sin * torch.sin(phi[:, i])\n",
    "    output[:, -1] = r * sin\n",
    "\n",
    "    # Split real/imag components and reshape to image format\n",
    "    real_output = output[:,0,:]\n",
    "    imaginary_output = output[:,1,:]\n",
    "    cartesian_output = torch.cat((real_output,imaginary_output),axis=1)\n",
    "    cartesian_output = torch.moveaxis(cartesian_output,1,3)\n",
    "    return cartesian_output\n",
    "\n",
    "#(numerically) stabilize angle; uses same epsilon as default for Torch's Adam optimizer\n",
    "#set all imaginary components out-of-range to epsilon\n",
    "def angle(z, epsilon=1e-8):\n",
    "    y = z.clone()\n",
    "    y.imag[(z.imag<epsilon)&(z.imag>-1.0*epsilon)] = epsilon\n",
    "    return y.angle()\n",
    "\n",
    "#use K-means clustering on model output; requires knowing how many distinct objects are in the\n",
    "#image; convert model output from polar to cartesian coordinates then cluster the imaginary components\n",
    "def cluster(tensor,k,mask):\n",
    "    imaginary = to_cartesian(tensor,mask).detach().cpu().numpy()\n",
    "    imaginary_shaped = imaginary.reshape(imaginary.shape[0],imaginary.shape[-2]*imaginary.shape[-3],imaginary.shape[-1])\n",
    "    clustered_images = []\n",
    "    for image in imaginary_shaped:\n",
    "        #KMeans on each image\n",
    "        kmeans = KMeans(n_clusters=k,n_init=5).fit(image).labels_\n",
    "        clustered_images.append(kmeans.reshape(imaginary.shape[-2],imaginary.shape[-3]))\n",
    "    labelled_images = np.array(clustered_images)\n",
    "    return labelled_images\n",
    "\n",
    "#compute average ARI score\n",
    "def ari_score(ground_truths,final_reconstructions,num_objects):\n",
    "    total_ari = 0\n",
    "    clustered_reconstructions = cluster(final_reconstructions,num_objects,ground_truths)\n",
    "    for i in range(final_reconstructions.shape[0]):\n",
    "        total_ari+=ari(clustered_reconstructions[i],ground_truths[i][0],num_objects)\n",
    "    return total_ari/clustered_reconstructions.shape[0]\n",
    "\n",
    "#convert data to tensors and normalize input data\n",
    "def convert_data(all_images,all_masks):\n",
    "    all_images = torch.tensor(all_images,dtype=torch.float64)/255.0\n",
    "    all_images = torch.moveaxis(all_images,3,1)\n",
    "    all_masks = torch.tensor(all_masks,dtype=torch.uint8)\n",
    "    return all_images,all_masks\n",
    "\n",
    "#visualization for demo\n",
    "def visualize(image,clustering,reconstruction,ground_truth,penultimate_output):\n",
    "    fig = plt.figure(figsize=(15, 3))\n",
    "\n",
    "    col1 = fig.add_subplot(151)\n",
    "    col1.axis(\"off\")\n",
    "    col1.set_title(\"Input image\")\n",
    "    col1.imshow(image)\n",
    "\n",
    "    col2 = fig.add_subplot(152)\n",
    "    col2.axis(\"off\")\n",
    "    col2.set_title(\"Ground truth\")\n",
    "    col2.imshow(ground_truth)\n",
    "\n",
    "    col3 = fig.add_subplot(153)\n",
    "    col3.axis(\"off\")\n",
    "    col3.set_title(\"Reconstruction\")\n",
    "    col3.imshow(reconstruction)\n",
    "\n",
    "    #Polar plot to visualize distribution of complex phases\n",
    "    col4 = fig.add_subplot(154,projection='polar')\n",
    "    col4.set_title(\"Polar output\")\n",
    "    theta = penultimate_output.angle().cpu().numpy().flatten()\n",
    "    r = penultimate_output.abs().cpu().numpy().flatten()\n",
    "    col4.scatter(theta,r,c=theta,cmap='hsv')\n",
    "\n",
    "    col5 = fig.add_subplot(155)\n",
    "    col5.axis(\"off\")\n",
    "    col5.set_title(\"Clustered phases\")\n",
    "    col5.imshow(clustering)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "#calculate adjusted rand index (ARI)\n",
    "def ari(clustering,ground_truth,num_objects):\n",
    "    contingencies =  np.zeros((num_objects, num_objects))\n",
    "    for i in range(clustering.shape[0]):\n",
    "        for j in range(clustering.shape[1]):\n",
    "            contingencies[clustering[i][j]][int(ground_truth[i][j].item()-1)]+=1\n",
    "    nij = np.sum([(val)/2*(val-1) for val in (contingencies)])\n",
    "    ai = sum([(val)/2*(val-1) for val in np.sum(contingencies,axis=1)])\n",
    "    bj = sum([(val)/2*(val-1) for val in np.sum(contingencies,axis=0)])\n",
    "    total = sum([val for val in np.sum(contingencies,axis=1)])\n",
    "    n = (total/2)*(total-1)\n",
    "    numerator = nij-(ai*bj)/n\n",
    "    denominator = (ai+bj)/2 -(ai*bj)/n\n",
    "    return(numerator/denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758aa001-c224-43ca-abdc-9c896365bc39",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<p style=\"font-size: 18px;\"><b>Dataset construction</b></p><br>\n",
    "\n",
    "To evaluate the model’s capacity for unsupervised object binding, we generated our own synthetic dataset of simple geometric scenes. Each image contains exactly three objects: a circle, a square, and a triangle, each with randomized size, position, and color texture. The dataset enforces several forms of variability:\n",
    "<ul>\n",
    "<li>Shape variation: Each object’s size is randomly selected from three possible values.</li>\n",
    "<li>Position variation: Objects are placed at random coordinates within a 35×35 frame.</li>\n",
    "<li>Texture variation: Each object receives random per-pixel RGB perturbations to prevent trivial color-based segmentation.</li>\n",
    "<li>Occlusion: Depth ordering is randomized among six permutations so objects can partially cover one another.</li>\n",
    "</ul>\n",
    "<br>\n",
    "Together, these characteristics allow the dataset to mimic important qualities of real object-centric datasets such as Tetrominoes (no occlusion) and CLEVR (occasional significant occlusion). These latter two datasets were two of the three datasets used in the source paper.\n",
    "<br><br>\n",
    "Each sample consists of:\n",
    "<ul>\n",
    "<li>A 35×35 RGB image</li>\n",
    "<li>A 35×35 mask labeling each pixel as background, circle, square, or triangle (4 total classes).</li>\n",
    "</ul>\n",
    "<br>\n",
    "The following code is used to generate the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289560ff-015a-44b6-9958-78d8bbb65a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dataset builder\n",
    "# Generates a synthetic dataset of circles, squares, triangles\n",
    "# with random size/position and randomized depth ordering.\n",
    "# Used to evaluate whether the model can recover object identity.\n",
    "# ============================================================\n",
    "\n",
    "#draw circle\n",
    "def circle(rgb_image,ground_truth,i,j,circle_image):\n",
    "    if circle_image[i][j]==1.0:\n",
    "        rgb_image[0,i,j]=255\n",
    "        rgb_image[1,i,j]=np.random.randint(low=100, high=200)\n",
    "        rgb_image[2,i,j]=np.random.randint(low=100, high=200)\n",
    "        ground_truth[i][j]=1\n",
    "    return rgb_image,ground_truth\n",
    "\n",
    "#draw square\n",
    "def square(rgb_image,ground_truth,i,j,square_parameters):\n",
    "    square_size = square_parameters[0]\n",
    "    square_upper_x = square_parameters[1]\n",
    "    square_upper_y = square_parameters[2]\n",
    "    if j>=square_upper_x and j<=square_upper_x+square_size:\n",
    "        if i>=square_upper_y and i<=square_upper_y+square_size:\n",
    "            rgb_image[0,i,j]=np.random.randint(low=100, high=200)\n",
    "            rgb_image[1,i,j]=255\n",
    "            rgb_image[2,i,j]=np.random.randint(low=100, high=200)\n",
    "            ground_truth[i][j]=2\n",
    "\n",
    "    return rgb_image,ground_truth\n",
    "\n",
    "#draw triangle\n",
    "def triangle(rgb_image,ground_truth,i,j,triangle_parameters):\n",
    "    triangle_size = triangle_parameters[0]\n",
    "    triangle_upper_x = triangle_parameters[1]\n",
    "    triangle_upper_y = triangle_parameters[2]\n",
    "    if j>=triangle_upper_x and j<=triangle_upper_x+triangle_size:\n",
    "        if i>=triangle_upper_y and i<=triangle_upper_y+triangle_size:\n",
    "            if j-triangle_upper_x<=i-triangle_upper_y:\n",
    "                rgb_image[0,i,j]=np.random.randint(low=100, high=200)\n",
    "                rgb_image[1,i,j]=np.random.randint(low=100, high=200)\n",
    "                rgb_image[2,i,j]=255\n",
    "                ground_truth[i][j]=3\n",
    "\n",
    "    return rgb_image,ground_truth\n",
    "\n",
    "#helper function used to handle depth permutations (e.g. square can go in front of circle or vice versa)\n",
    "def return_shape(rgb_image,ground_truth,ordering,i,j,circle_image,square_parameters,triangle_parameters):\n",
    "    for k in range(3):\n",
    "        if ordering[k]==\"c\":\n",
    "            rgb_image,ground_truth = circle(rgb_image,ground_truth,i,j,circle_image)\n",
    "        if ordering[k]==\"s\":\n",
    "            rgb_image,ground_truth = square(rgb_image,ground_truth,i,j,square_parameters)\n",
    "        if ordering[k]==\"t\":\n",
    "            rgb_image,ground_truth = triangle(rgb_image,ground_truth,i,j,triangle_parameters)\n",
    "    return rgb_image,ground_truth\n",
    "\n",
    "#main dataset generation loop\n",
    "def generate_dataset(num_samples):\n",
    "\n",
    "    sizes = [4,8,14]\n",
    "\n",
    "    resolution = (35,35)\n",
    "\n",
    "    images = []\n",
    "    masks = []\n",
    "    counter = 0\n",
    "    while counter<num_samples:\n",
    "\n",
    "        rgb_image = np.random.randint(low=0, high=25, size=(3,resolution[0],resolution[1]))\n",
    "        circle_size = sizes[np.random.randint(0,3)]\n",
    "        circle_mid_x = np.random.randint(circle_size-1,resolution[0]-circle_size-1)\n",
    "        circle_mid_y = np.random.randint(circle_size-1,resolution[0]-circle_size-1)\n",
    "\n",
    "        square_size = sizes[np.random.randint(0,3)]\n",
    "        square_upper_x = np.random.randint(0,resolution[0]-square_size-1)\n",
    "        square_upper_y = np.random.randint(0,resolution[0]-square_size-1)\n",
    "\n",
    "        triangle_size = sizes[np.random.randint(0,3)]\n",
    "        triangle_upper_x = np.random.randint(0,resolution[0]-triangle_size-1)\n",
    "        triangle_upper_y = np.random.randint(0,resolution[0]-triangle_size-1)\n",
    "\n",
    "        x, y = np.meshgrid(np.arange(resolution[0]), np.arange(resolution[1]))\n",
    "\n",
    "        squared_distance = (x - circle_mid_x)**2 + (y - circle_mid_y)**2\n",
    "\n",
    "        ground_truth = np.zeros(resolution,dtype=np.uint8)\n",
    "        circle_mask = squared_distance <= (circle_size/2)**2\n",
    "\n",
    "        circle_image=np.zeros(resolution)\n",
    "        circle_image[circle_mask]=1.0\n",
    "        permutation = np.random.randint(0,6)\n",
    "\n",
    "        square_parameters = [square_size,square_upper_x,square_upper_y]\n",
    "        triangle_parameters = [triangle_size,triangle_upper_x,triangle_upper_y]\n",
    "        for i in range(resolution[0]):\n",
    "            for j in range(resolution[1]):\n",
    "\n",
    "                if permutation==0:\n",
    "                    ordering = [\"c\",\"s\",\"t\"]\n",
    "                    rgb_image,ground_truth = return_shape(rgb_image,ground_truth,ordering,i,j,circle_image,square_parameters,triangle_parameters)\n",
    "                elif permutation==1:\n",
    "                    ordering = [\"c\",\"t\",\"s\"]\n",
    "                    rgb_image,ground_truth = return_shape(rgb_image,ground_truth,ordering,i,j,circle_image,square_parameters,triangle_parameters)\n",
    "                elif permutation==2:\n",
    "                    ordering = [\"s\",\"c\",\"t\"]\n",
    "                    rgb_image,ground_truth = return_shape(rgb_image,ground_truth,ordering,i,j,circle_image,square_parameters,triangle_parameters)\n",
    "                elif permutation==3:\n",
    "                    ordering = [\"s\",\"t\",\"c\"]\n",
    "                    rgb_image,ground_truth = return_shape(rgb_image,ground_truth,ordering,i,j,circle_image,square_parameters,triangle_parameters)\n",
    "                elif permutation==4:\n",
    "                    ordering = [\"t\",\"c\",\"s\"]\n",
    "                    rgb_image,ground_truth = return_shape(rgb_image,ground_truth,ordering,i,j,circle_image,square_parameters,triangle_parameters)\n",
    "                elif permutation==5:\n",
    "                    ordering = [\"t\",\"s\",\"c\"]\n",
    "                    rgb_image,ground_truth = return_shape(rgb_image,ground_truth,ordering,i,j,circle_image,square_parameters,triangle_parameters)\n",
    "\n",
    "        if len(np.unique(ground_truth))==4:\n",
    "            images.append(np.moveaxis(rgb_image,0,2))\n",
    "            masks.append(ground_truth)\n",
    "            counter+=1\n",
    "\n",
    "    return np.array(images).astype(np.uint8),np.array(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec16a41-b550-4f61-b908-92727ac440d4",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<p style=\"font-size: 18px;\"><b>Dataset sample</b></p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207747e-902e-48fb-a84a-29e4227b8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstrate dataset images\n",
    "#display 2 images and their corresponding masks\n",
    "\n",
    "num_samples = 2\n",
    "sample_images,sample_masks = generate_dataset(num_samples)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 3))\n",
    "\n",
    "col1 = fig.add_subplot(141)\n",
    "col1.axis(\"off\")\n",
    "col1.set_title(\"Sample image 1\")\n",
    "col1.imshow(sample_images[0])\n",
    "\n",
    "col2 = fig.add_subplot(142)\n",
    "col2.axis(\"off\")\n",
    "col2.set_title(\"Sample mask 1\")\n",
    "col2.imshow(sample_masks[0])\n",
    "\n",
    "col3 = fig.add_subplot(143)\n",
    "col3.axis(\"off\")\n",
    "col3.set_title(\"Sample image 2\")\n",
    "col3.imshow(sample_images[1])\n",
    "\n",
    "col3 = fig.add_subplot(144)\n",
    "col3.axis(\"off\")\n",
    "col3.set_title(\"Sample mask 2\")\n",
    "col3.imshow(sample_masks[1])\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0131e104-2fbb-47cf-b12c-4056c2fb832e",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<p style=\"font-size: 20px;\"><b>Model definition</b></p><br>\n",
    "<br>\n",
    "<p style=\"font-size: 18px;\"><b>Layers</b></p><br>\n",
    "The complex-weighted autoencoder requires using custom layers in order to work with complex numbers. The following is a brief description of how these layers function:\n",
    "<ul>\n",
    "<li><b>complex_convolution():</b> Performs 2D convolution on complex-valued input. Weight initialization for the magnitude component uniformly samples from a Rayleigh distribution with $\\sigma = \\frac{1}{fan-in}$ while sampling the phase component uniformly from [$-\\pi$,$\\pi$]. Magnitude biases are initialized uniformly from [$-\\frac{1}{\\sqrt{fan-in}}$,$\\frac{1}{\\sqrt{fan-in}}$] while phase biases are simply initialized to $0$. Note that fan-in is the number of input units to a particular layer. The forward method for complex_convolution() numerically stablizes the output from convolution and returns $\\mu e^{i\\phi}$ where $\\mu$ is the magnitude component of the output and $\\phi$ is the stablized phase component of the output.</li>\n",
    "<li><b>complex_upsample():</b>Performs nearest-neighbors upsampling on the phase and magnitude components separately and combines them by returning $\\mu e^{i\\phi}$ similar to <b>complex_convolution()</b>.</li>\n",
    "<li><b>modrelu():</b> The activation function used which adapts ReLU to work with complex numbers. It returns <b>ReLU</b>($\\mu+$bias)$\\odot e^{i\\phi}$ where $\\mu$ is the magnitude component of the input and $\\phi$ is the phase component of the input.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3e497-c31f-47d4-aeae-c84ddfba2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Complex-valued CNN Layer Definitions\n",
    "# These layers operate directly on complex tensors:\n",
    "# - complex_convolution: complex Conv2d with magnitude/phase bias\n",
    "# - complex_upsample: upsampling magnitude + phase separately\n",
    "# - modrelu: activation applied to magnitude, preserves phase\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "#class for custom convolution with complex numbers\n",
    "class complex_convolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Complex-valued convolution block.\n",
    "    Weight initialized in polar form.\n",
    "    Output = (|conv| + mag_bias) * exp(i*(angle(conv) + phase_bias))\n",
    "    \"\"\"\n",
    "    #initialize complex convolution layer\n",
    "    def __init__(self,input_size,output_size,kernel,stride,padding,init_phase_min,init_phase_max):\n",
    "        super().__init__()\n",
    "        #initialize convolution parameters\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.input_size,self.output_size = input_size,output_size\n",
    "        self.initial_phase_minimum = init_phase_min\n",
    "        self.initial_phase_maximum = init_phase_max\n",
    "        #initialize torch Conv2d\n",
    "        self.convolution = nn.Conv2d(self.input_size,\n",
    "                                     self.output_size,\n",
    "                                     self.kernel,\n",
    "                                     self.stride,\n",
    "                                     padding,\n",
    "                                     bias=False,\n",
    "                                     dtype=torch.complex64)\n",
    "        self.convolution.apply(self._initialize_weights)\n",
    "        self.magnitude_bias = nn.Parameter(torch.empty((1,self.output_size,1,1)))\n",
    "        self.phase_bias = nn.Parameter(torch.empty((1,self.output_size,1,1)))\n",
    "        fan_in = self.input_size * self.kernel**2\n",
    "        self.magnitude_bias,self.phase_bias = self._initialize_biases(fan_in,self.magnitude_bias,self.phase_bias)\n",
    "\n",
    "    #initialize complex weights\n",
    "    def _initialize_weights(self,module):\n",
    "        magnitude = np.random.rayleigh(1/np.sqrt(np.prod(module.weight.shape[1:])), module.weight.shape)\n",
    "        phase = np.random.uniform(self.initial_phase_minimum, self.initial_phase_maximum,module.weight.shape)\n",
    "        with torch.no_grad():\n",
    "            module.weight.real.copy_(torch.from_numpy(magnitude * np.cos(phase)))\n",
    "            module.weight.imag.copy_(torch.from_numpy(magnitude * np.sin(phase)))\n",
    "    #initialize complex biases\n",
    "    def _initialize_biases(self,fan_in,magnitude_bias,phase_bias):\n",
    "        #edit\n",
    "        nn.init.uniform_(magnitude_bias, -1/np.sqrt(fan_in), 1/np.sqrt(fan_in))\n",
    "        nn.init.constant_(phase_bias, val=0)\n",
    "        return magnitude_bias,phase_bias\n",
    "    #forward method for complex convolution\n",
    "    def forward(self, x):\n",
    "        complex_conv = self.convolution(x)\n",
    "        magnitude_component = complex_conv.abs() + self.magnitude_bias\n",
    "        #use numerically stable phase\n",
    "        phase_component = angle(complex_conv) + self.phase_bias\n",
    "        return magnitude_component * torch.exp(phase_component * 1j)\n",
    "\n",
    "class complex_upsample(nn.Module):\n",
    "    \"\"\"Nearest-neighbor upsampling for magnitude & phase separately.\"\"\"\n",
    "    def __init__(self, size):\n",
    "        super(complex_upsample, self).__init__()\n",
    "        #edit\n",
    "        self.upsample_magnitude = nn.UpsamplingNearest2d(size)\n",
    "        self.upsample_phase = nn.UpsamplingNearest2d(size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        magnitude_component = self.upsample_magnitude(x.abs())\n",
    "        phase_component = self.upsample_phase(x.angle())\n",
    "        return magnitude_component * torch.exp(phase_component * 1j)\n",
    "\n",
    "class modrelu(nn.Module):\n",
    "    \"\"\"ModReLU activation used in complex-valued networks.\"\"\"\n",
    "    def __init__(self,hidden_dimensions):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.Tensor(1,hidden_dimensions,1,1))\n",
    "        nn.init.constant_(self.bias, -0.1)\n",
    "    def forward(self, x):\n",
    "        return nn.functional.relu(x.abs() + self.bias) * (torch.exp(x.angle() * 1j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e6cf3-93b2-4baa-8eed-45899cf23d7c",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style=\"font-size: 18px;\"><b>Architecture</b></p><br>\n",
    "The implementation follows the architecture from the source paper fairly closely though scales it down. The following table describes the architecture of the complex-weighted autoencoder:<br>\n",
    "\n",
    "|          | Original implementation | Scaled down implementation |\n",
    "|:--------|:--------:|:--------:|\n",
    "|  <b>Encoder:</b>   |     |     |\n",
    "|  <b>Encoder layer 1:</b> Input resolution: 35x35; output resolution: 17x17<ul><li>Complex convolution: kernel size: 3x3; stride: 2; input channels:</li><li>modReLU</li><li>Function: Spatially downsample complex input while increasing number of feature maps; apply nonlinear activation to complex output</li><uk>   |  input channels: 3<br>output channels: 64   |  input channels: 3<br>output channels: 32   |\n",
    "|  <b>Encoder layer 2:</b> Input resolution: 17x17; output resolution: 8x8<ul><li>Complex convolution: kernel size: 3x3; stride: 2; input channels:</li><li>modReLU</li><li>Function: Spatially downsample complex input while increasing number of feature maps; apply nonlinear activation to complex output</li><uk>   |  input channels: 64<br>output channels: 128   |  input channels: 32<br>output channels: 64   |\n",
    "|  <b>Encoder layer 3:</b> Input resolution: 8x8; output resolution: 3x3<ul><li>Complex convolution: kernel size: 3x3; stride: 2; input channels:</li><li>modReLU</li><li>Function: Spatially downsample complex input while preserving number of feature maps; apply nonlinear activation to complex output</li><uk>   |  input channels: 128<br>output channels: 128   |  input channels: 64<br>output channels: 64   |\n",
    "|  <b>Decoder:</b>   |     |     |\n",
    "|  <b>Decoder layer 1:</b> Input resolution: 3x3; output resolution: 8x8<ul><li>Nearest neighbors upsampling</li><li>Complex convolution: kernel size: 3x3; stride: 2; input channels:</li><li>modReLU</li><li>Function: Spatially upsample complex input while preserving number of feature maps; apply complex convolution preserving increased spatial resolution; apply nonlinear activation to complex output</li><uk>   |  input channels: 128<br>output channels: 128   |  input channels: 64<br>output channels: 64   |\n",
    "|  <b>Decoder layer 2:</b> Input resolution: 8x8; output resolution: 17x17<ul><li>Nearest neighbors upsampling</li><li>Complex convolution: kernel size: 3x3; stride: 2; input channels:</li><li>modReLU</li><li>Function: Spatially upsample complex input while reducing number of feature maps; apply complex convolution preserving increased spatial resolution; apply nonlinear activation to complex output</li><uk>   |  input channels: 128<br>output channels: 64   |  input channels: 64<br>output channels: 32   |\n",
    "|  <b>Decoder layer 3:</b> Input resolution: 17x17; output resolution: 35x35<ul><li>Nearest neighbors upsampling</li><li>Complex convolution: kernel size: 3x3; stride: 2; input channels:</li><li>modReLU</li><li>Function: Spatially upsample complex input while reducing number of feature maps; apply complex convolution preserving increased spatial resolution; apply nonlinear activation to complex output</li><uk>   |  input channels: 64<br>output channels: 3   |  input channels: 32<br>output channels: 3   |\n",
    "|  <b>Output head:</b> Input resolution: 35x35; output resolution: 35x35<ul><li>Complex convolution: kernel size: 1x1; stride: 1</li><li>Function: Apply output complex convolution while preserving number of feature maps and spatial dimensions</li><uk>   |  input channels: 3<br>output channels: 3   |  input channels: 3<br>output channels: 3   |\n",
    "\n",
    "The following code defines the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d6871-a7dd-4c5c-89e2-5d3d9ac3516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# CAE Encoder–Decoder Architecture\n",
    "# 3 downsampling encoder stages and 3 upsampling decoder stages.\n",
    "#\n",
    "# Each encoder stage uses:\n",
    "# - a complex convolution\n",
    "# - followed by modReLU\n",
    "#\n",
    "# Each decoder stage uses:\n",
    "# - nearest neighbors upsampling\n",
    "# - a complex convolution\n",
    "# - followed by modReLU\n",
    "#\n",
    "# Encoder structure (example):\n",
    "#   conv -> modrelu -> conv -> modrelu -> conv -> modrelu\n",
    "#\n",
    "# Decoder reconstructs input back symmetrically.\n",
    "# Decoder structure (example):\n",
    "#   upsample -> conv -> modrelu -> upsample -> conv -> modrelu\n",
    "#\n",
    "# ==============================================================\n",
    "\n",
    "\n",
    "class cae_model(nn.Module):\n",
    "    \"\"\"\n",
    "    Main encoder-decoder backbone.\n",
    "    - Encoder: complex conv + modReLU\n",
    "    - Decoder: complex upsample + complex conv + modReLU\n",
    "    Uses fixed architecture defined by encoder/decoder_parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self,input_size,output_size,hidden_units,resolution,kernel,stride,init_phase_min,init_phase_max):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.resolution = resolution\n",
    "        self.padding = (self.kernel-self.stride)//2\n",
    "        self.init_phase_min=init_phase_min\n",
    "        self.init_phase_max=init_phase_max\n",
    "\n",
    "        # Encoder structure: (channels_in_multiplier, channels_out_multiplier)\n",
    "        self.encoder_parameters = [[1,1],[1,2],[2,2]]\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        for j,k in self.encoder_parameters:\n",
    "            input_dims = self.input_size if k==1 else self.output_size\n",
    "\n",
    "            self.encoder_blocks.append(complex_convolution(j*self.hidden_units if k>1 else self.input_size,\n",
    "                                                           k*self.hidden_units,\n",
    "                                                           self.kernel,\n",
    "                                                           self.stride,\n",
    "                                                           \"same\" if stride==1 else \"valid\",\n",
    "                                                           self.init_phase_min,\n",
    "                                                           self.init_phase_max))\n",
    "            self.encoder_blocks.append(modrelu(k*self.hidden_units))\n",
    "\n",
    "        # Mirror decoder architecture\n",
    "        self.decoder_parameters = [[i]+self.encoder_parameters[i] for i in range(3)][::-1]\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "\n",
    "        for i,j,k in self.decoder_parameters:\n",
    "            input_dims = self.input_size if k==1 else self.output_size\n",
    "            self.decoder_blocks.append(complex_upsample(self.resolution[0]//self.stride**i))\n",
    "            self.decoder_blocks.append(complex_convolution(k*self.hidden_units,\n",
    "                                                           j*self.hidden_units if k>1 else self.input_size,\n",
    "                                                           self.kernel,\n",
    "                                                           1,\n",
    "                                                           \"same\",\n",
    "                                                           self.init_phase_min,\n",
    "                                                           self.init_phase_max))\n",
    "            self.decoder_blocks.append(modrelu(j*self.hidden_units if k>1 else self.input_size))\n",
    "        if self.resolution[0] in [64,94]:\n",
    "            self.decoder_blocks.append(complex_convolution(self.input_size,self.output_size,1,1,\"same\",self.init_phase_min,self.init_phase_max))\n",
    "            self.decoder_blocks.append(modrelu(j*self.hidden_units if k>1 else self.input_size))\n",
    "            \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Run through encoder blocks then decoder blocks.\n",
    "        Returns every intermediate activation (for visualization).\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for index, encoder_block in enumerate(self.encoder_blocks):\n",
    "            x = encoder_block(x)\n",
    "            output.append(x)\n",
    "        for index, decoder_block in enumerate(self.decoder_blocks):\n",
    "            x = decoder_block(x)\n",
    "            output.append(x)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21c6269e-2b78-4667-a296-4b6978fd03fc",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style=\"font-size: 18px;\"><b>Model class</b></p><br>\n",
    "The main model class initializes the architecture. It also converts raw image input to complex input. It does this by sampling phases randomly from a Von Mises distribution with $\\mu = 0$ and concentration $= 1$. It returns : $\\mu e^{i\\phi}$ where $\\mu$ is the raw input and $\\phi$ is the sampled phases. The forward method works as follows:<br>\n",
    "Forward:\n",
    "<ul>\n",
    "<li>Step 0: convert raw input to complex input</li>\n",
    "<li>Do steps 1-5 n times:</li>\n",
    "<li>Step 1: pass converted input to model</li>\n",
    "<li>Step 2: pass model body output to model head</li>\n",
    "<li>Step 3: Use model head phase output to reconvert raw input to complex input</li>\n",
    "<li>Step 4: retain the model body outputs and model head outputs</li>\n",
    "<li>Step 5: go to step 1</li>\n",
    "<li>Step 6: convert all model head inputs to reconstucted images using output magnitude</li>\n",
    "<li>Step 7: return all model body outputs image reconstructions</li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "<b>Figure 4</b> below illustrates at a high-level how the model functions. \n",
    "<center>\n",
    "<img src=\"https://lh3.googleusercontent.com/d/1OG5fu5PC0-_yih6TuSV45dwnn3ZLBImE\" style=\"display:block; margin:auto\"/>\n",
    "<b>Figure 4.</b> Note that the phase output only appears in the form shown above after K-Means clustering.</center><br><br>\n",
    "The reuse of model outputs to reprocess the same inputs n times is what makes this a recurrent model. The model uses a loss function which computes the mean squared error over each of the n passes. This encourages pixel-wise fidelity in magnitude while allowing phase to adapt naturally as a binding mechanism. Consistent with the original paper, the phase is not included in the loss; it emerges from the dynamics of the reconstruction process. The loss is defined as:<br><br>\n",
    "<center>\n",
    "<b>L</b> $=\\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{n}\\sum_{i=1}^{n} ||\\mu_{xj} - \\mu_{zj}^{i}||^{2}$\n",
    "</center>\n",
    "<br><br>\n",
    "m denotes the number of images, x denotes the raw input image, and z denotes the model reconstructions\n",
    "<br>\n",
    "It should be noted that when computing the loss, the raw input image is repeated n times for each of the n recurrent outputs.\n",
    "<br>\n",
    "The following code defines the main model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ea841-5e65-4873-86cb-af4f9eae52d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Main Recurrent Complex Autoencoder\n",
    "# ----------------------------------------------------------------------------------\n",
    "# - Runs CAE body for N iterations\n",
    "# - Each iteration predicts a phase map\n",
    "# - First phase map is randomly initialized\n",
    "# - New complex input is constructed using the predicted phase from prior iteration\n",
    "# - Final activations are used for clustering\n",
    "# ==================================================================================\n",
    "\n",
    "class complex_autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Full model with recurrence:\n",
    "      inputs -> (make complex) -> [CAE] -> output\n",
    "      Phase of output is fed back into next iteration (z)\n",
    "    \"\"\"\n",
    "    def __init__(self,img_resolution,input_size,kernel,stride,hidden_units,iterations,phase_init_min,phase_init_max):\n",
    "        super(complex_autoencoder, self).__init__()\n",
    "        self.resolution = img_resolution\n",
    "        self.input_size = input_size\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.output_size = self.input_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.iterations = iterations\n",
    "        self.init_phase_min = phase_init_min\n",
    "        self.init_phase_max = phase_init_max\n",
    "\n",
    "        self.model_body = cae_model(self.input_size,\n",
    "                                    self.output_size,\n",
    "                                    self.hidden_units,\n",
    "                                    self.resolution,\n",
    "                                    self.kernel,\n",
    "                                    self.stride,\n",
    "                                    self.init_phase_min,\n",
    "                                    self.init_phase_max)\n",
    "        self.model_output = complex_convolution(self.input_size,self.output_size,1,1,\"same\",self.init_phase_min,self.init_phase_max)\n",
    "\n",
    "    #randomly initialize phase map and use to convert input to complex input\n",
    "    def _make_complex_input(self,input_features):\n",
    "        von_mises_distribution = torch.distributions.von_mises.VonMises(loc=torch.tensor(0.),concentration=torch.tensor(1.))\n",
    "        phase = von_mises_distribution.sample(input_features.shape).to(device)\n",
    "        complex_input = input_features * torch.exp(phase * 1j)\n",
    "        return complex_input.to(torch.complex64)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "        For n iterations:\n",
    "          - Run encoder-decoder backbone\n",
    "          - Produce head output\n",
    "          - Re-inject phase back into input\n",
    "        Returns:\n",
    "          - all reconstructions (magnitudes)\n",
    "          - intermediate complex outputs (for clustering)\n",
    "        \"\"\"\n",
    "        outputs = torch.empty(0,dtype=torch.complex64).to(device)\n",
    "        intermediate_outputs = torch.empty(0,dtype=torch.complex64).to(device)\n",
    "        z = self._make_complex_input(inputs)\n",
    "        for n in range(self.iterations):\n",
    "            body_output = self.model_body(z)\n",
    "            head_output = self.model_output(body_output[-1])\n",
    "            outputs=torch.cat((outputs,torch.unsqueeze(head_output,0)),axis=0)\n",
    "            intermediate_outputs=torch.cat((intermediate_outputs,torch.unsqueeze(body_output[-1],0)),axis=0)\n",
    "            z = (inputs * torch.exp(head_output.angle() * 1j)).to(torch.complex64)\n",
    "        outputs = torch.moveaxis(outputs,0,1)\n",
    "        intermediate_outputs = torch.moveaxis(intermediate_outputs,0,1)\n",
    "        all_reconstructions = outputs.abs().to(torch.float64)\n",
    "        return all_reconstructions, intermediate_outputs\n",
    "\n",
    "    #compute MSE over each of the n recurrent passes (mean MSE)\n",
    "    def get_loss(self,reconstructions,inputs):\n",
    "        inputs = torch.unsqueeze(inputs,1)\n",
    "        #repeat input for each of the n passes\n",
    "        inputs = inputs.repeat(1, reconstructions.shape[1], 1, 1, 1)\n",
    "        loss = torch.nn.functional.mse_loss(reconstructions, inputs, reduction=\"mean\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097f62b-255e-40ac-87e9-44bd14cb3c58",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style=\"font-size: 18px;\"><b>Generate dataset</b></p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75ffdc-7d77-49d0-a2f0-a33f4e3b85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dataset Loading & Splitting\n",
    "# ------------------------------------------------------------\n",
    "# - Generate synthetic dataset\n",
    "# - Convert images/masks to tensors\n",
    "# - Split into training / validation / test sets\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "total_samples = 1550\n",
    "all_images,all_masks = generate_dataset(total_samples)\n",
    "\n",
    "#Convert and split\n",
    "images, masks = convert_data(all_images,all_masks)\n",
    "training_images,training_masks = images[:1000],masks[:1000]\n",
    "validation_images,validation_masks = images[1000:1500],masks[1000:1500]\n",
    "testing_images,testing_masks = images[1500:1550],masks[1500:1550]\n",
    "\n",
    "#Merge masks with images for validation loader\n",
    "validation_masks = torch.unsqueeze(validation_masks,axis=1)\n",
    "testing_masks = torch.unsqueeze(testing_masks,axis=1)\n",
    "validation_images = torch.cat((validation_images,validation_masks),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0297c48-51fd-4712-8d29-8db698334583",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style=\"font-size: 18px;\"><b>Model training</b></p><br>\n",
    "The training procedure for our scaled-down recurrent complex-weighted autoencoder closely follows the principles introduced in the SynCx model. This section describes (1) preprocessing and data splits, (2) the training objective and forward pass, and (3) the validation strategy including ARI-based early stopping. The overarching goal of training is to reconstruct the input magnitudes while allowing the model's complex phases to self-organize into object bindings without any supervision.<br><br>\n",
    "\n",
    "We generated 1550 samples and used the following split:\n",
    "<ul>\n",
    "<li>Training: 1000 samples</li>\n",
    "<li>Validation: 500 samples</li>\n",
    "<li>Test: 50 samples</li>\n",
    "</ul>\n",
    "\n",
    "The validation set concatenates images and masks along the channel dimension to facilitate ARI computation during training.\n",
    "\n",
    "<p style=\"font-size: 16px;\">Training objective:</p>\n",
    "As in the SynCx model, the network is not trained to predict object masks or phases directly. Instead, the autoencoder is trained only to reconstruct the magnitude of the input image. The complex phase is left entirely unsupervised; it is expected to self-organize into coherent synchrony groups that represent object bindings.<br><br>\n",
    "For each minibatch:<br>\n",
    "&emsp;1. The real input image is converted to a complex input by assigning a randomly sampled phase from a Von Mises distribution.<br>\n",
    "&emsp;2. The model performs n recurrent iterations, each producing a reconstruction and a phase map.<br>\n",
    "&emsp;3. In each iteration, the new complex input is created by combining the original magnitude with the predicted phase.<br>\n",
    "\n",
    "<br>\n",
    "<p style=\"font-size: 16px;\">Training loop structure:</p>\n",
    "Each training epoch consists of:<br><br>\n",
    "1. <b>Forward Pass</b><br>\n",
    "The model returns:\n",
    "<ul>\n",
    "<li>all_reconstructions: magnitude outputs for all recurrent iterations</li>\n",
    "<li>intermediate_outputs: the final complex activations (used for clustering)</li>\n",
    "</ul>\n",
    "2. <b>Loss Computation and Backpropagation</b><br>\n",
    "&emsp;Reconstruction loss is computed using the averaged magnitude MSE.<br>\n",
    "&emsp;Gradients are backpropagated through the recurrent pathway.<br>\n",
    "&emsp;Gradient clipping (clip_grad_norm_) is used for numerical stability.<br>\n",
    "&emsp;The optimizer (Adam, lr = 0.0005, $\\epsilon$ = 1e-8) performs an update step.<br>\n",
    "<br>\n",
    "Random seeds are fixed for NumPy and PyTorch to ensure determinism.\n",
    "If running on CUDA, deterministic cuDNN behavior is enabled as well.\n",
    "<br>\n",
    "Both training and validation use infinite iterators (cycle()) so that evaluation timings are independent of dataset size and do not require epoch alignment.\n",
    "<br><br>\n",
    "<p style=\"font-size: 16px;\">Validation and ARI-Based Early Stopping:</p>\n",
    "While training is driven purely by magnitude MSE, performance is evaluated using the Adjusted Rand Index (ARI), which measures consistency between predicted phase clusters and the ground-truth object masks. This evaluation reflects the true purpose of the model—object binding via synchrony among phases.<br><br>\n",
    "\n",
    "Every evaluation_trigger (10) training epochs:<br>\n",
    "&emsp;1. A validation batch is drawn.<br>\n",
    "&emsp;2. The model performs a full forward pass.<br>\n",
    "&emsp;3. The validation loss is computed.<br>\n",
    "&emsp;4. The phase output from the final iteration is clustered using K-Means.<br>\n",
    "&emsp;5. ARI is computed between the cluster labels and the ground-truth mask.<br>\n",
    "<br>\n",
    "The model uses an ARI-based early stopping mechanism:\n",
    "\n",
    "<ul>\n",
    "<li>If validation ARI improves, the model is checkpointed.</li>\n",
    "<li>If ARI does not improve for patience = 30 evaluation rounds, training stops.</li>\n",
    "<li>The best checkpoint is re-loaded after early stopping.</li>\n",
    "</ul>\n",
    "\n",
    "This strategy aligns with the methodology of the SynCx paper: reconstruction loss shapes the magnitude, but binding quality is be measured by phase coherence, not reconstruction error.\n",
    "\n",
    "<p style=\"font-size: 16px;\">Final Model Selection:</p>\n",
    "After training stops, the best model checkpoint is loaded and evaluated on the held-out test set. For the final model:\n",
    "<ul>\n",
    "<li>Test MSE reflects reconstruction fidelity.</li>\n",
    "<li>If ARI does not improve for patience = 30 evaluation rounds, training stops.</li>\n",
    "<li>Test ARI evaluates object binding accuracy using phase clustering.</li>\n",
    "</ul>\n",
    "\n",
    "The following code runs the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12217062-6b11-45ce-9022-418f701853a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Training loop with early stopping\n",
    "# ============================================================\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "#dataset parameters\n",
    "resolution = (35,35)\n",
    "input_channels = 3\n",
    "num_objects = 4\n",
    "\n",
    "#model and training parameters\n",
    "hidden_units = 32\n",
    "n_iterations = 3\n",
    "init_phase_min = -np.pi\n",
    "init_phase_max = np.pi\n",
    "learning_rate = .0005\n",
    "epsilon = 1e-8\n",
    "epochs = 1000\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "\n",
    "#other training parameters\n",
    "batch_size = 64\n",
    "evaluation_trigger = 10\n",
    "progress_trigger = 25\n",
    "\n",
    "# ============================================================\n",
    "# Model + Optimizer\n",
    "# ============================================================\n",
    "model = complex_autoencoder(resolution,\n",
    "                            input_channels,\n",
    "                            kernel_size,\n",
    "                            stride,\n",
    "                            hidden_units,\n",
    "                            n_iterations,\n",
    "                            init_phase_min,\n",
    "                            init_phase_max).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate,\n",
    "                             eps=epsilon)\n",
    "\n",
    "# DataLoaders (infinite iterator using cycle())\n",
    "train_dataloader = torch.utils.data.DataLoader(training_images, batch_size=batch_size, shuffle=True)\n",
    "train_iterator = cycle(train_dataloader)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_images, batch_size=batch_size, shuffle=True)\n",
    "validation_iterator = cycle(validation_dataloader)\n",
    "\n",
    "# ============================================================\n",
    "# Training loop with ARI-based early stopping\n",
    "# ============================================================\n",
    "\n",
    "losses = []\n",
    "validation_scores = []\n",
    "validation_losses = []\n",
    "\n",
    "# Early stopping settings\n",
    "patience = 30                          # Number of validation cycles to wait without improvement\n",
    "best_ari = -float(\"inf\")               # Best ARI observed so far\n",
    "patience_counter = 0                   # Number of successive non-improving evaluations\n",
    "best_model_state = None                # To store best model weights\n",
    "\n",
    "print(\"Model training:\")\n",
    "print()\n",
    "\n",
    "#Key idea:\n",
    "# - Train every epoch\n",
    "# - Every evaluation_trigger epochs: run validation\n",
    "# - Track best ARI, save checkpoint\n",
    "# - Stop if no improvement for 'patience' intervals\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    inputs = next(train_iterator).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    all_reconstructions, intermediate_outputs = model(inputs)\n",
    "    loss = model.get_loss(all_reconstructions, inputs)\n",
    "\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if (epoch+1)%progress_trigger==0:\n",
    "        print(\"Epoch \"+str(epoch+1)+\" loss (MSE): \"+str(loss.item()))\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Perform validation only at set intervals\n",
    "    if (epoch + 1) % evaluation_trigger == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            validation_data = next(validation_iterator).to(device)\n",
    "            validation_images_ = validation_data[:, :3, :, :]\n",
    "            validation_masks_ = validation_data[:, 3:, :, :]\n",
    "\n",
    "            all_reconstructions, intermediate_outputs = model(validation_images_)\n",
    "            val_loss = model.get_loss(all_reconstructions, validation_images_)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "            # Compute validation ARI\n",
    "            current_ari = ari_score(validation_masks_, intermediate_outputs[:,-1], num_objects)\n",
    "            validation_scores.append(current_ari)\n",
    "\n",
    "            # Early stopping check\n",
    "            if current_ari > best_ari:\n",
    "                best_ari = current_ari\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "                # Save checkpoint to file\n",
    "                torch.save(best_model_state, \"best_model.pth\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Stop if no improvement for too long\n",
    "            if patience_counter >= patience:\n",
    "                model.load_state_dict(best_model_state)\n",
    "                print(\"Early stopping applied. Best model saved.\")\n",
    "                break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device)); None\n",
    "model.to(device); None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff321d70-0dc6-4ea8-9fb4-6b9f59cb3eb7",
   "metadata": {},
   "source": [
    "<br><p style=\"font-size: 18px;\"><b>Training metrics</b></p><br>\n",
    "The following code displays the training and validation MSE and ARI curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79135ff9-a9cd-4d82-a0ae-dc5db90a0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Plot training curves:\n",
    "# - Training MSE\n",
    "# - Validation MSE\n",
    "# - Validation ARI\n",
    "# ============================================================\n",
    "\n",
    "#display training metrics\n",
    "train_epoch = [ep+1 for ep in range(epoch+1)]\n",
    "val_epoch = [epoch+1 for epoch in range(len(validation_scores))]\n",
    "\n",
    "fig, (col1, col2, col3) = plt.subplots(1, 3,figsize=(15,5))\n",
    "col1.set_title(\"Training loss (MSE):\")\n",
    "col1.set_xlabel(\"Training Epoch\")\n",
    "col1.set_ylabel(\"MSE\")\n",
    "col2.set_title(\"Validation loss (MSE):\")\n",
    "col2.set_xlabel(\"Validation Iteration\")\n",
    "col2.set_ylabel(\"MSE\")\n",
    "col3.set_title(\"Validation average ARI:\")\n",
    "col3.set_xlabel(\"Validation Iteration\")\n",
    "col3.set_ylabel(\"Average ARI\")\n",
    "col1.plot(train_epoch,losses)\n",
    "col2.plot(val_epoch,validation_losses)\n",
    "col3.plot(val_epoch,validation_scores)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e54400-33c3-4ec9-a438-fdf38fb482d5",
   "metadata": {},
   "source": [
    "<br><p style=\"font-size: 18px;\"><b>Test set evaluation</b></p><br>\n",
    "The following code evaluates the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736d47d-ec1e-4006-9b29-9676396abd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Final evaluation on unseen test split.\n",
    "# ARI measures segmentation consistency.\n",
    "# ============================================================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_reconstructions, intermediate_outputs = model(testing_images.to(device))\n",
    "    test_loss = model.get_loss(all_reconstructions,testing_images.to(device))\n",
    "    test_average_ari = ari_score(testing_masks.to(device), intermediate_outputs[:,-1],num_objects)\n",
    "\n",
    "print(\"Test set loss (MSE): \"+str(test_loss.item()))\n",
    "print(\"Test set score (ARI) :\"+str(test_average_ari))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e073c9b-0b94-4491-8b72-377e3e78ba91",
   "metadata": {},
   "source": [
    "<br><p style=\"font-size: 18px;\"><b>Results of training and testing</b></p><br>\n",
    "The numeric results show the training reduced MSE from its starting value of about 0.1618 to an ending value of about 0.0295. The training and validation curves are relatively smooth and have the sort of shapes one would hope for to indicate model learning. On the hold-out test set, the model achieved a MSE of about 0.0315 and an ARI score of about 0.9761. The ARI score was particularly good and was better than that reported by the paper. This may have been because the toy dataset was simpler than those used in the original paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c4171-efaf-4062-a306-7a443903a175",
   "metadata": {},
   "source": [
    "<br><p style=\"font-size: 18px;\"><b>Demonstration 1</b></p><br>\n",
    "The following illustrates functionality of the trained model on four sample images from the test set. Each demonstration sample shows:<br>\n",
    "<ul>\n",
    "<li>The raw image</li>\n",
    "<li>The ground truth mask</li>\n",
    "<li>The reconstructed image</li>\n",
    "<li>The model's phase outputs</li>\n",
    "<li>The clustered phase outputs</li>\n",
    "</ul>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0eb56-6640-4eaf-8b12-13ea8ba197a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Demo #1:\n",
    "# - Show reconstruction for first 4 test samples\n",
    "# - Display phase clustering + polar plots\n",
    "# ============================================================\n",
    "\n",
    "demo1_samples = 4\n",
    "with torch.no_grad():\n",
    "    all_reconstructions, intermediate_outputs = model(testing_images[:demo1_samples].to(device))\n",
    "    clustered_reconstructions = cluster(intermediate_outputs[:,-1],num_objects,testing_masks[:demo1_samples].to(device))\n",
    "    for i in range(demo1_samples):\n",
    "        print(\"Demo 1 sample \"+str(i+1)+\":\")\n",
    "        reconstruction = all_reconstructions[i][-1].abs()\n",
    "        reconstruction = torch.moveaxis(reconstruction,0,2).cpu().numpy()\n",
    "        reconstruction = np.clip(reconstruction,0,1)\n",
    "        image = testing_images[:demo1_samples][i]\n",
    "        image = torch.moveaxis(image,0,2).cpu().numpy()\n",
    "        visualize(image,clustered_reconstructions[i],reconstruction,testing_masks[i][0],intermediate_outputs[:,-1][i][-1])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f4cc2-6b4f-4545-95e6-3e389ca26882",
   "metadata": {},
   "source": [
    "<br><p style=\"font-size: 18px;\"><b>Demonstration 2</b></p><br>\n",
    "The following illustrates how the model's phase outputs evolve over each of the n recurrent iterations on two sample images from the test set.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9f231-cd0d-4627-8de8-2474a657669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Demo #2:\n",
    "# - Show clustering evolution across recurrent iterations\n",
    "# - Helps visualize how phase changes over iterations\n",
    "# ============================================================\n",
    "\n",
    "demo2_samples = 2\n",
    "with torch.no_grad():\n",
    "    demo2_images = testing_images[demo1_samples:demo1_samples+demo2_samples].to(device)\n",
    "    demo2_masks = testing_masks[demo1_samples:demo1_samples+demo2_samples].to(device)\n",
    "    all_reconstructions, intermediate_outputs = model(demo2_images)\n",
    "\n",
    "\n",
    "    for i in range(demo2_samples):\n",
    "        print(\"Demo 2 sample \"+str(i+1)+\":\")\n",
    "        repeated_mask = demo2_masks[i].repeat(n_iterations,1,1)\n",
    "        clustered_reconstructions = cluster(intermediate_outputs[i],num_objects,repeated_mask)\n",
    "\n",
    "        image = demo2_images[i]\n",
    "        image = torch.moveaxis(image,0,2).cpu().numpy()\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 3))\n",
    "        n = 100 + 10*(n_iterations+1)\n",
    "        col1 = fig.add_subplot(n+1)\n",
    "        col1.axis(\"off\")\n",
    "        col1.set_title(\"Input\\nimage\")\n",
    "        col1.imshow(image)\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            col = fig.add_subplot(n+i+2)\n",
    "            col.axis(\"off\")\n",
    "            col.set_title(\"Clustered phases\\niteration \"+str(i+1))\n",
    "            col.imshow(clustered_reconstructions[i])\n",
    "\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72b4b4-15ab-4069-ae44-011fd74fbaca",
   "metadata": {},
   "source": [
    "<br><p style=\"font-size: 24px;\"><b>Discussion</b></p>\n",
    "As can be seen from the demonstrations, the model was far from perfect. It could have likely benefited from more epochs and hidden units. It appears based on the demonstration images that the model struggles when objects occlude one another or are touching. This can be seen in the first demonstration's third sample where the small square was significantly obstructed by the triangle - the model wound up treating the square as part of the triangle. Another case of this was the second demonstration's first image where the model got the depth ordering of the circle and the square reversed. It appears to struggle with the boundaries between objects. The best of the demonstration images appear to be those where the shapes are separated. The first demonstration's fourth image or the second demonstration's second image are examples of this. Also, all demonstration images except the first demonstration's third image has each object with a majority distinct color which is a good sign as can be interpreted as a measure of confidence in the distinctness of each object. The scaled-down model struggled with occlusion, though it should be noted that the larger model in the paper appeared to as well. The author's implementation had the best ARI on Tetronimoes where the objects do not overlap, while it did the worst on CLEVR where overlap is possible.  \n",
    "\n",
    "Despite not being perfect, the results were promising with an excellent average ARI score on the test set and relatively smooth training and validation ARI and MSE curves. The image reconstructions were not very good, but this was somewhat expected given the low epoch count and scaled-down size. Additionally, image reconstruction was not the point of the model - the point was to obtain object bindings from phase output. The demonstration polar plots of the phase output were not great either - they appear worse than their corresponding phase clustering would suggest. The ideal polar plots would have shown four well-separated columnar formations of points each corresponding to a distinct object plus one for the background. That is because object bindings are obtained by phase alignment/synchrony so each of the four ideally distinct columns would represent a group of aligned phases.\n",
    "\n",
    "There are multiple avenues for future work based on this model. It would be interesting to see what would happen if the number of feature maps were reduced in the encoder rather than increased. The authors chose a structure which expanded the number of feature maps in the middle, and it would be interesting to know what results the inverse structure would yield. Based not upon the scaled-down implementation, but upon the paper's parameters, it would be interesting to know what impact more layers and/or hidden units would have. It would also be interesting to know what effect different kernel sizes, strides, or even dilated convolution would have. Since the model struggled with occlusion, it would be interesting to know whether there are any preprocessing filters which could sharpen object contrast or otherwise help it distinguish nearby or visually overlapping objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543559f8-fd7b-4bc6-9f78-c47cfa9403f8",
   "metadata": {},
   "source": [
    "<br><p style=\"font-size: 18px;\"><b>References</b></p>\n",
    "\n",
    "Gopalakrishnan, Anand, Aleksandar Stanić, Jürgen Schmidhuber, and Michael Curtis Mozer.\n",
    "2024. “Recurrent Complex-Weighted Autoencoders for Unsupervised Object Discovery.” In\n",
    "Advances in Neural Information Processing Systems, edited by A. Globerson, L. Mackey, D.\n",
    "Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, 37:140787–811. Curran Associates,\n",
    "Inc. <a href=\"https://www.proceedings.com/079017-4468.html\">https://doi.org/10.52202/079017-4468</a>. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
