{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9915e827-e167-40de-b170-78a96f2402e2",
   "metadata": {},
   "source": [
    "The following cell defines the functions needed for demonstrating the pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f094c-68e2-419c-b2a6-29cd33b0c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, activations\n",
    "from keras.initializers import RandomNormal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#format generator output for display\n",
    "def format_image(generator_output):\n",
    "    unscaled_image = generator_output[0,:,:,:]\n",
    "    scaled_image = (generator_output*127.5)+127.5\n",
    "    clipped_image = np.clip(scaled_image,0.0,255.0)\n",
    "    clipped_image/=255.0\n",
    "    return clipped_image[0]\n",
    "\n",
    "#display generated images\n",
    "def display_images(images):\n",
    "\n",
    "    for idx in range(len(images)):\n",
    "        img = format_image(images[idx])\n",
    "        plt.imshow(img)\n",
    "        plt.title('Test image #'+str(idx+1))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "#make generator following architecture outlined in paper\n",
    "def make_generator_model():\n",
    "\n",
    "    #input layer takes noise vector\n",
    "    input_layer = keras.Input(shape=(100,))\n",
    "\n",
    "    #fully connected layer without bias or activation to reshape input\n",
    "    fcl = layers.Dense(4*4*3, use_bias=False,name='fcl')(input_layer)\n",
    "\n",
    "    #reshape fully connected output\n",
    "    reshape = layers.Reshape((4, 4, 3),name='reshape')(fcl)\n",
    "\n",
    "    #transpose convolution block 1 using 512 filters, (mu=0,std=.02) normal initialization, batch normalization, and relu activation\n",
    "    tconv1 = layers.Conv2DTranspose(512,3,padding='same',kernel_initializer=RandomNormal(mean=0.0, stddev=0.02),name='tconv1')(reshape)\n",
    "    bnorm1 = layers.BatchNormalization()(tconv1)\n",
    "    relu1 = layers.Activation(activations.relu)(bnorm1)\n",
    "\n",
    "    #transpose convolution block 2 using 256 filters, (mu=0,std=.02) normal initialization, batch normalization, and relu activation\n",
    "    tconv2 = layers.Conv2DTranspose(256,3,strides=2,padding='same',kernel_initializer=RandomNormal(mean=0.0, stddev=0.02),name='tconv2')(relu1)\n",
    "    bnorm2 = layers.BatchNormalization()(tconv2)\n",
    "    relu2 = layers.Activation(activations.relu)(bnorm2)\n",
    "\n",
    "    #self attention layer\n",
    "    attention = layers.MultiHeadAttention(1,256,kernel_initializer=RandomNormal(mean=0.0, stddev=0.02),name='attention')(relu2,relu2)\n",
    "\n",
    "    #transpose convolution block 3 using 128 filters, (mu=0,std=.02) normal initialization, batch normalization, and relu activation\n",
    "    tconv3 = layers.Conv2DTranspose(128,3,strides=2,padding='same',kernel_initializer=RandomNormal(mean=0.0, stddev=0.02),name='tconv3')(attention)\n",
    "    bnorm3 = layers.BatchNormalization()(tconv3)\n",
    "    relu3 = layers.Activation(activations.relu)(bnorm3)\n",
    "\n",
    "    #transpose convolution block 4 using 64 filters, (mu=0,std=.02) normal initialization, batch normalization, and relu activation\n",
    "    tconv4 = layers.Conv2DTranspose(64,3,strides=2,padding='same',kernel_initializer=RandomNormal(mean=0.0, stddev=0.02),name='tconv4')(tconv3)\n",
    "    bnorm4 = layers.BatchNormalization()(tconv4)\n",
    "    relu4 = layers.Activation(activations.relu)(bnorm4)\n",
    "\n",
    "    #transpose convolution block 5 (output layer) using 3 filters, (mu=0,std=.02) normal initialization, batch normalization, and relu activation\n",
    "    output_layer = layers.Conv2DTranspose(3,3,strides=2,padding='same',kernel_initializer=RandomNormal(mean=0.0, stddev=0.02),activation='tanh',name='output')(relu4)\n",
    "\n",
    "    #assemble model\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer,name='generator')\n",
    "\n",
    "    return model\n",
    "\n",
    "#make discriminator following architecture outlined in paper\n",
    "def make_discriminator_model():\n",
    "\n",
    "    #input layer taking output from discriminator\n",
    "    input_layer = keras.Input(shape=[64, 64, 3])\n",
    "\n",
    "    #spectral normalization layer with 64 filters and leaky relu activation\n",
    "    spec_norm = layers.SpectralNormalization(layers.Conv2D(64,3,strides=2,padding='same',name='spec_norm'))(input_layer)\n",
    "    lrelu1 = layers.LeakyReLU(name='lrelu1')(spec_norm)\n",
    "\n",
    "    #convolutional block 1 with 128 filters and leaky relu activation\n",
    "    conv1 = layers.Conv2D(128,3,strides=2,padding='same',name='conv1')(lrelu1)\n",
    "    bnorm1 = layers.BatchNormalization()(conv1)\n",
    "    lrelu2 = layers.LeakyReLU(name='lrelu2')(bnorm1)\n",
    "\n",
    "    #convolutional block 2 with 256 filters and leaky relu activation\n",
    "    conv2 = layers.Conv2D(256,3,strides=2,padding='same',name='conv2')(lrelu2)\n",
    "    bnorm2 = layers.BatchNormalization()(conv2)\n",
    "    lrelu3 = layers.LeakyReLU(name='lrelu3')(conv2)\n",
    "\n",
    "    #convolutional block 3 with 512 filters and leaky relu activation\n",
    "    conv3 = layers.Conv2D(512,3,strides=2,padding='same',name='conv3')(lrelu3)\n",
    "    bnorm3 = layers.BatchNormalization()(conv3)\n",
    "    lrelu4 = layers.LeakyReLU(name='lrelu4')(conv3)\n",
    "\n",
    "    #flatten convolutional block output\n",
    "    flattened = layers.Flatten()(lrelu4)\n",
    "\n",
    "    #spectral normalization output layer with (mu=0,std=.02) normal initialization, one unit and sigmoid activation (binary classification for real vs fake)\n",
    "    output_layer = layers.SpectralNormalization(layers.Dense(1,kernel_initializer=RandomNormal(mean=0.0, stddev=0.02),activation='sigmoid',name='output_layer'))(flattened)\n",
    "\n",
    "    #assemble model\n",
    "    model = keras.Model(inputs=input_layer,outputs=output_layer,name='discriminator')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cda992-2f54-42fd-bd5c-6ca4a3265a31",
   "metadata": {},
   "source": [
    "The following cell allows using the model trained on the Anime Faces dataset to generate sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e114812-3c9e-42fe-bee3-5280f4342e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please change to the path of the animefacedataset generator weights \n",
    "animefacedataset_generator_weights_path = 'animefacedataset_generator.weights.h5'\n",
    "#please change to the path of the animefacedataset discriminator weights\n",
    "animefacedataset_discriminator_weights_path = 'animefacedataset_discriminator.weights.h5'  \n",
    "\n",
    "#make generator\n",
    "animefacedataset_generator = make_generator_model()\n",
    "#make discriminator\n",
    "animefacedataset_discriminator = make_discriminator_model()\n",
    "\n",
    "#load generator weights\n",
    "animefacedataset_generator.load_weights(animefacedataset_generator_weights_path)\n",
    "#load discriminator weights\n",
    "animefacedataset_discriminator.load_weights(animefacedataset_discriminator_weights_path)\n",
    "\n",
    "#number of images to generate - please change to desired number\n",
    "animefacedataset_NUM_TEST_IMAGES = 4\n",
    "animefacedataset_test_images = []\n",
    "\n",
    "#generate images\n",
    "for idx in range(animefacedataset_NUM_TEST_IMAGES):\n",
    "    noise = tf.random.normal([1, 100])\n",
    "    generated_image = animefacedataset_generator(noise, training=False)\n",
    "    animefacedataset_test_images.append(generated_image)\n",
    "\n",
    "#display images\n",
    "display_images(animefacedataset_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f9f8d-e508-448d-8f10-370fe541b96a",
   "metadata": {},
   "source": [
    "The following cell allows using the model trained on the celebA dataset to generate sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35771bb9-efe7-4d1e-a524-8a5d660c7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please change to the path of the celebA generator weights \n",
    "celebA_generator_weights_path = 'celebA_generator.weights.h5'\n",
    "#please change to the path of the celebA discriminator weights\n",
    "celebA_discriminator_weights_path = 'celebA_discriminator.weights.h5'  \n",
    "\n",
    "#make generator\n",
    "celebA_generator = make_generator_model()\n",
    "#make discriminator\n",
    "celebA_discriminator = make_discriminator_model()\n",
    "\n",
    "#load generator weights\n",
    "celebA_generator.load_weights(celebA_generator_weights_path)\n",
    "#load discriminator weights\n",
    "celebA_discriminator.load_weights(celebA_discriminator_weights_path)\n",
    "\n",
    "#number of images to generate - please change to desired number\n",
    "celebA_NUM_TEST_IMAGES = 4\n",
    "celebA_test_images = []\n",
    "\n",
    "#generate images\n",
    "for idx in range(celebA_NUM_TEST_IMAGES):\n",
    "    noise = tf.random.normal([1, 100])\n",
    "    generated_image = celebA_generator(noise, training=False)\n",
    "    celebA_test_images.append(generated_image)\n",
    "\n",
    "#display images\n",
    "display_images(celebA_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436374b2-1cab-45a2-9079-3189de234243",
   "metadata": {},
   "source": [
    "The following cell allows using the model trained on the iCartoonFace dataset to generate sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d911b5d-2bbe-4604-8f50-735c045100ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please change to the path of the iCartoonFace generator weights \n",
    "iCartoonFace_generator_weights_path = 'iCartoonFace_generator.weights.h5'\n",
    "#please change to the path of the iCartoonFace discriminator weights\n",
    "iCartoonFace_discriminator_weights_path = 'iCartoonFace_discriminator.weights.h5'  \n",
    "\n",
    "#make generator\n",
    "iCartoonFace_generator = make_generator_model()\n",
    "#make discriminator\n",
    "iCartoonFace_discriminator = make_discriminator_model()\n",
    "\n",
    "#load generator weights\n",
    "iCartoonFace_generator.load_weights(iCartoonFace_generator_weights_path)\n",
    "#load discriminator weights\n",
    "iCartoonFace_discriminator.load_weights(iCartoonFace_discriminator_weights_path)\n",
    "\n",
    "#number of images to generate - please change to desired number\n",
    "iCartoonFace_NUM_TEST_IMAGES = 4\n",
    "iCartoonFace_test_images = []\n",
    "\n",
    "#generate images\n",
    "for idx in range(iCartoonFace_NUM_TEST_IMAGES):\n",
    "    noise = tf.random.normal([1, 100])\n",
    "    generated_image = iCartoonFace_generator(noise, training=False)\n",
    "    iCartoonFace_test_images.append(generated_image)\n",
    "\n",
    "#display images\n",
    "display_images(iCartoonFace_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d034a1d-cf61-4dab-8c7f-2105e1dd6c84",
   "metadata": {},
   "source": [
    "The following cell allows using the model trained on the LFW dataset to generate sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae595d3b-04e8-4b93-ac7d-d4517d03750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#please change to the path of the lfw_dataset generator weights \n",
    "lfw_dataset_generator_weights_path = 'lfw_dataset_generator.weights.h5'\n",
    "#please change to the path of the lfw_dataset discriminator weights\n",
    "lfw_dataset_discriminator_weights_path = 'lfw_dataset_discriminator.weights.h5'  \n",
    "\n",
    "#make generator\n",
    "lfw_dataset_generator = make_generator_model()\n",
    "#make discriminator\n",
    "lfw_dataset_discriminator = make_discriminator_model()\n",
    "\n",
    "#load generator weights\n",
    "lfw_dataset_generator.load_weights(lfw_dataset_generator_weights_path)\n",
    "#load discriminator weights\n",
    "lfw_dataset_discriminator.load_weights(lfw_dataset_discriminator_weights_path)\n",
    "\n",
    "#number of images to generate - please change to desired number\n",
    "lfw_dataset_NUM_TEST_IMAGES = 4\n",
    "lfw_dataset_test_images = []\n",
    "\n",
    "#generate images\n",
    "for idx in range(lfw_dataset_NUM_TEST_IMAGES):\n",
    "    noise = tf.random.normal([1, 100])\n",
    "    generated_image = lfw_dataset_generator(noise, training=False)\n",
    "    lfw_dataset_test_images.append(generated_image)\n",
    "\n",
    "#display images\n",
    "display_images(lfw_dataset_test_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
